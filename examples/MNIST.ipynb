{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to MyVision\n",
    "This is a library I made to combine everything I <3 about PyTorch.\n",
    "My goal is \"Do more with less code\". Well `MyVision` is a wrapper over PyTorch.\n",
    "That means u must know PyTorch before working with it and if u know PyTorch you can yourself make any customizations. Just have at look at the source code on github.\n",
    "\n",
    "With this aside let's start our example. It's the MNIST example as u might have guessed already :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard \"Every ML/DL problem\" imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let me give u a brief overview of what `MyVision` offers:\n",
    "There are two important things at the heart of it.\n",
    "1. Dataset\n",
    "2. Trainer\n",
    "<br>\n",
    "The former one we will go through in another example.\n",
    "Here, we go through `Trainer`\n",
    "\n",
    "So what is `Trainer`?\n",
    "Simply, Trainer provides trainig and validation methods, normally in PyTorch you have to write your\n",
    "custom loop, which let me tell you, gives you ultimate customization. But I wanted to do something like what keras `.fit()` offers. So I decided to build it up.\n",
    "\n",
    "Trainer offers you this keras like `.fit()` magic. With proper parameters you can simply `.fit()` and *boom!* training begins.\n",
    "\n",
    "So, let's import the specifics. \n",
    "Our `Trainer` is present in `MyVision.engine.Engine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MyVision\n",
    "from MyVision.dataset.Dataset import DatasetUtils\n",
    "from MyVision.engine.Engine import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we just make the two DataLoaders because as you know in PyTorch `DataLoader` is where the heavylifting takes place. Our trainer expects these DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "    batch_size=128, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "    batch_size=128, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we do usual stuff i.e. define our `model`, `optimizer` & `loss` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = torch.nn.Linear(in_features=model.fc.in_features, out_features=10)\n",
    "\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = False\n",
    "# for p in model.fc.parameters():\n",
    "#     p.requires_grad = True\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the part to what we have been building upto aka `Trainer`.\n",
    "Let's have a look at what functions does the `Trainer` has.\n",
    "Run the cell below to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[1;31mSignature:\u001b[0m\n\u001b[0mTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mmetric_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'models'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0maccumulation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mDocstring:\u001b[0m <no docstring>\n\u001b[1;31mFile:\u001b[0m      e:\\library\\myvision\\engine\\engine.py\n\u001b[1;31mType:\u001b[0m      function\n"
    }
   ],
   "source": [
    "?Trainer.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that out `Trainer` just takes in the usual stuff:\n",
    "1. Training, Validation & Test(if specified) DataLoaders\n",
    "2. device(either `cpu` or `cuda`)\n",
    "3. loss\n",
    "4. optimizer\n",
    "5. model\n",
    "6. learning rate scheduler(if you want)\n",
    "<br>\n",
    "Whatever you don't want just specify it as `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for the magic to begin specifiy number of epochs and the scheduler metric in the `.fit()`\n",
    "Now just run the cell below and we are off !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100%|██████████| 469/469 [01:10<00:00,  6.62it/s]\n100%|██████████| 79/79 [00:03<00:00, 23.36it/s]\n[SAVING] to models\\best_model-(0).pth.tar\n+-------+------------+-----------------+----------+\n| Epoch | Train loss | Validation loss | accuracy |\n+-------+------------+-----------------+----------+\n|   1   |   0.606    |      0.182      |  0.9459  |\n+-------+------------+-----------------+----------+\n100%|██████████| 469/469 [01:09<00:00,  6.76it/s]\n100%|██████████| 79/79 [00:03<00:00, 23.42it/s]\n[SAVING] to models\\best_model-(1).pth.tar\n+-------+------------+-----------------+----------+\n| Epoch | Train loss | Validation loss | accuracy |\n+-------+------------+-----------------+----------+\n|   1   |   0.606    |      0.182      |  0.9459  |\n|   2   |   0.149    |      0.112      |  0.9657  |\n+-------+------------+-----------------+----------+\n100%|██████████| 469/469 [01:07<00:00,  6.95it/s]\n100%|██████████| 79/79 [00:03<00:00, 24.22it/s]\n[SAVING] to models\\best_model-(2).pth.tar\n+-------+------------+-----------------+----------+\n| Epoch | Train loss | Validation loss | accuracy |\n+-------+------------+-----------------+----------+\n|   1   |   0.606    |      0.182      |  0.9459  |\n|   2   |   0.149    |      0.112      |  0.9657  |\n|   3   |    0.09    |      0.093      |  0.9703  |\n+-------+------------+-----------------+----------+\n100%|██████████| 469/469 [01:05<00:00,  7.16it/s]\n100%|██████████| 79/79 [00:03<00:00, 24.61it/s]\n[SAVING] to models\\best_model-(3).pth.tar\n+-------+------------+-----------------+----------+\n| Epoch | Train loss | Validation loss | accuracy |\n+-------+------------+-----------------+----------+\n|   1   |   0.606    |      0.182      |  0.9459  |\n|   2   |   0.149    |      0.112      |  0.9657  |\n|   3   |    0.09    |      0.093      |  0.9703  |\n|   4   |   0.064    |      0.082      |  0.9758  |\n+-------+------------+-----------------+----------+\n100%|██████████| 469/469 [01:05<00:00,  7.19it/s]\n100%|██████████| 79/79 [00:03<00:00, 24.03it/s]\n[SAVING] to models\\best_model-(4).pth.tar\n+-------+------------+-----------------+----------+\n| Epoch | Train loss | Validation loss | accuracy |\n+-------+------------+-----------------+----------+\n|   1   |   0.606    |      0.182      |  0.9459  |\n|   2   |   0.149    |      0.112      |  0.9657  |\n|   3   |    0.09    |      0.093      |  0.9703  |\n|   4   |   0.064    |      0.082      |  0.9758  |\n|   5   |   0.045    |      0.076      |  0.9771  |\n+-------+------------+-----------------+----------+\n"
    }
   ],
   "source": [
    "Trainer.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    device='cuda',\n",
    "    criterion=loss,\n",
    "    optimizer=optimizer,\n",
    "    model=model.to('cuda'),\n",
    "    lr_scheduler=None,\n",
    "    metric_name='accuracy',\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you must have seen most of it just regular PyTorch stuff but with lots of convinience. If you know PyTorch it's just a breeze for you to understand :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also `MyVision` creates a models folder in the directory where you run the script and saves the best models to it epoch-wise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}